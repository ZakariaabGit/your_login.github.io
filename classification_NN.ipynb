{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD_classif_correction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZakariaabGit/zakariaabGit.github.io/blob/main/classification_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ur3h07r7X1Y0",
        "outputId": "e34e5095-e157-423f-e302-900f4ebf29fd"
      },
      "source": [
        "!pip install tensorflow-text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed tensorflow-text-2.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWaCty8k8faB"
      },
      "source": [
        "import numpy as np\n",
        "#from collections import Counter\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.layers import Input, LSTM, GRU, SimpleRNN, Masking, Embedding, Dense, Flatten\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cfR7Fik77Ii"
      },
      "source": [
        "# 1 Les données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYYY3Km58Chh"
      },
      "source": [
        "La base de données utilisée ici est la base IMDB, très connue en analyse de texte. Il s’agit\n",
        "d’avis sur des films qui sont classés comme \"positifs\" (classe 1) ou \"négatifs\" (classe 0).\n",
        "Le jeu de données est composé de 25000 avis positifs et 25000 avis négatifs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7Vq1FgrQDFE"
      },
      "source": [
        "def get_texts_and_labels(data):\n",
        "  texts, labels = [], []\n",
        "  for text, label in data:\n",
        "    texts.append(text.numpy().decode('utf-8'))\n",
        "    labels.append(label.numpy())\n",
        "  return texts, labels\n",
        "\n",
        "def load_data():\n",
        "\n",
        "  train_data = tfds.load(\n",
        "    'imdb_reviews',\n",
        "    split='train',\n",
        "    # batch_size=BATCH_SIZE,  # None \n",
        "    shuffle_files=True,\n",
        "    as_supervised=True)\n",
        "  \n",
        "  test_data = tfds.load(\n",
        "    'imdb_reviews',\n",
        "    split='test',\n",
        "    # batch_size=BATCH_SIZE,  # None\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True)\n",
        "\n",
        "  return train_data, test_data\n",
        "\n",
        "\n",
        "train_data, test_data = load_data()\n",
        "train_texts, train_labels = get_texts_and_labels(train_data)\n",
        "test_texts, test_labels = get_texts_and_labels(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQNN9-KFX5sQ"
      },
      "source": [
        "train_texts[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CE4x-SGGcb93",
        "outputId": "4a2c481f-a6c4-403e-832c-b84e0a380416"
      },
      "source": [
        "len(train_texts), len(test_texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 25000)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFr7Pn8EdGPl"
      },
      "source": [
        "small_train_texts = train_texts[:5]\n",
        "small_train_labels = train_labels[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKH0rgWA8ZvA"
      },
      "source": [
        "# 2 Traitements préliminaires sur les données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRWv7BMl8epH"
      },
      "source": [
        "## 2.1 Observation du vocabulaire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAnf-ARL8h2P"
      },
      "source": [
        "Notez qu’un mot est uniquement défini par un espace : la présence\n",
        "d’une ponctuation derrière un mot créé des entités différentes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-V41_MiCfE_t",
        "outputId": "efb00b9b-e916-4215-a2cd-39925555a2b5"
      },
      "source": [
        "def get_words(lines):\n",
        "  dict_words = {}\n",
        "  index_words = {}\n",
        "  index = 1\n",
        "  for line in lines:\n",
        "    tokens = line.split()\n",
        "    for token in tokens:\n",
        "      if token in dict_words:\n",
        "        dict_words[token] += 1\n",
        "      else:\n",
        "        dict_words[token] = 1\n",
        "        index_words[token] = index\n",
        "        index += 1\n",
        "  return dict_words, index_words\n",
        "\n",
        "dict_words, index_words = get_words(small_train_texts)\n",
        "print(dict_words)\n",
        "print(len(dict_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'This': 3, 'was': 6, 'an': 2, 'absolutely': 2, 'terrible': 1, 'movie.': 1, \"Don't\": 1, 'be': 5, 'lured': 1, 'in': 7, 'by': 1, 'Christopher': 2, 'Walken': 2, 'or': 2, 'Michael': 1, 'Ironside.': 1, 'Both': 1, 'are': 6, 'great': 2, 'actors,': 1, 'but': 6, 'this': 8, 'must': 1, 'simply': 1, 'their': 3, 'worst': 1, 'role': 1, 'history.': 1, 'Even': 1, 'acting': 1, 'could': 2, 'not': 1, 'redeem': 1, \"movie's\": 1, 'ridiculous': 1, 'storyline.': 1, 'movie': 2, 'is': 3, 'early': 1, 'nineties': 1, 'US': 1, 'propaganda': 1, 'piece.': 1, 'The': 3, 'most': 1, 'pathetic': 2, 'scenes': 1, 'were': 2, 'those': 1, 'when': 3, 'the': 26, 'Columbian': 1, 'rebels': 1, 'making': 1, 'cases': 1, 'for': 8, 'revolutions.': 1, 'Maria': 1, 'Conchita': 1, 'Alonso': 1, 'appeared': 1, 'phony,': 1, 'and': 13, 'her': 1, 'pseudo-love': 1, 'affair': 1, 'with': 4, 'nothing': 1, 'a': 15, 'emotional': 1, 'plug': 1, 'that': 5, 'devoid': 1, 'of': 15, 'any': 2, 'real': 2, 'meaning.': 1, 'I': 7, 'am': 1, 'disappointed': 1, 'there': 1, 'movies': 1, 'like': 2, 'this,': 1, 'ruining': 1, \"actor's\": 1, \"Walken's\": 1, 'good': 1, 'name.': 1, 'barely': 1, 'sit': 1, 'through': 2, 'it.': 1, 'have': 4, 'been': 1, 'known': 1, 'to': 12, 'fall': 1, 'asleep': 2, 'during': 2, 'films,': 1, 'usually': 1, 'due': 1, 'combination': 1, 'things': 1, 'including,': 1, 'really': 2, 'tired,': 1, 'being': 1, 'warm': 2, 'comfortable': 1, 'on': 5, 'sette': 1, 'having': 1, 'just': 3, 'eaten': 1, 'lot.': 1, 'However': 1, 'occasion': 1, 'fell': 1, 'because': 1, 'film': 5, 'rubbish.': 1, 'plot': 3, 'development': 1, 'constant.': 1, 'Constantly': 1, 'slow': 1, 'boring.': 1, 'Things': 1, 'seemed': 2, 'happen,': 1, 'no': 4, 'explanation': 1, 'what': 1, 'causing': 1, 'them': 1, 'why.': 1, 'admit,': 1, 'may': 1, 'missed': 1, 'part': 1, 'film,': 1, 'i': 1, 'watched': 1, 'majority': 1, 'it': 3, 'everything': 1, 'happen': 1, 'its': 3, 'own': 2, 'accord': 1, 'without': 1, 'concern': 1, 'anything': 1, 'else.': 1, 'cant': 1, 'recommend': 1, 'at': 3, 'all.': 1, 'Mann': 2, 'photographs': 1, 'Alberta': 1, 'Rocky': 1, 'Mountains': 1, 'superb': 1, 'fashion,': 1, 'Jimmy': 1, 'Stewart': 1, 'Walter': 1, 'Brennan': 1, 'give': 1, 'enjoyable': 2, 'performances': 2, 'as': 2, 'they': 3, 'always': 1, 'seem': 1, 'do.': 1, '<br': 2, '/><br': 3, '/>But': 1, 'come': 1, 'Hollywood': 1, '-': 1, 'Mountie': 1, 'telling': 1, 'people': 1, 'Dawson': 2, 'City,': 1, 'Yukon': 1, 'elect': 1, 'themselves': 1, 'marshal': 1, '(yes': 1, 'marshal!)': 1, 'enforce': 1, 'law': 1, 'themselves,': 1, 'then': 2, 'gunfighters': 1, 'battling': 1, 'out': 1, 'streets': 1, 'control': 1, 'town?': 1, '/>Nothing': 1, 'even': 1, 'remotely': 1, 'resembling': 1, 'happened': 1, 'Canadian': 2, 'side': 1, 'border': 1, 'Klondike': 1, 'gold': 1, 'rush.': 1, 'Mr.': 1, 'company': 1, 'appear': 1, 'mistaken': 1, 'City': 1, 'Deadwood,': 1, 'North': 1, 'American': 1, 'Wild': 1, 'West.<br': 1, '/>Canadian': 1, 'viewers': 1, 'prepared': 1, 'Reefer': 1, 'Madness': 1, 'type': 1, 'howl': 1, 'ludicrous': 1, 'plot,': 1, 'or,': 1, 'shake': 1, 'your': 1, 'head': 1, 'disgust.': 1, 'kind': 1, 'snowy': 1, 'Sunday': 1, 'afternoon': 1, 'rest': 1, 'world': 1, 'can': 1, 'go': 2, 'ahead': 1, 'business': 1, 'you': 1, 'descend': 1, 'into': 1, 'big': 1, 'arm-chair': 1, 'mellow': 1, 'couple': 1, 'hours.': 1, 'Wonderful': 1, 'from': 1, 'Cher': 1, 'Nicolas': 1, 'Cage': 1, '(as': 1, 'always)': 1, 'gently': 1, 'row': 1, 'along.': 1, 'There': 1, 'rapids': 1, 'cross,': 1, 'dangerous': 1, 'waters,': 1, 'witty': 1, 'paddle': 1, 'New': 1, 'York': 1, 'life': 1, 'best.': 1, 'A': 1, 'family': 1, 'every': 1, 'sense': 1, 'one': 1, 'deserves': 1, 'praise': 1, 'received.': 1, 'As': 1, 'others': 1, 'mentioned,': 1, 'all': 2, 'women': 1, 'nude': 1, 'mostly': 1, 'gorgeous.': 1, 'very': 1, 'ably': 1, 'shows': 1, 'hypocrisy': 1, 'female': 1, 'libido.': 1, 'When': 1, 'men': 1, 'around': 1, 'want': 1, 'pursued,': 1, '\"men\"': 1, 'around,': 1, 'become': 1, 'pursuers': 1, '14': 1, 'year': 1, 'old': 1, 'boy.': 1, 'And': 1, 'boy': 1, 'becomes': 1, 'man': 1, 'fast': 1, '(we': 1, 'should': 1, 'so': 1, 'lucky': 1, 'age!).': 1, 'He': 1, 'gets': 1, 'up': 1, 'courage': 1, 'pursue': 1, 'his': 1, 'true': 1, 'love.': 1}\n",
            "333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kBoas2Z8l6B"
      },
      "source": [
        "## 2.2 Nettoyage des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hERaqj_68sTA"
      },
      "source": [
        "Notez que le nombre de mots sera réduit et que certains mots du lexique\n",
        "auront désormais une occurrence plus importante, ce qui sera bénéfique pour une\n",
        "modélisation automatique par une méthode d’apprentissage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOncuGCsPmne",
        "outputId": "7d5978f9-848f-4530-cb95-dcabd25fafe8"
      },
      "source": [
        "filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n\\''\n",
        "\n",
        "def clean_textlines(lines):\n",
        "  new_lines = []\n",
        "  for line in lines:\n",
        "\n",
        "    # suppression des éléments du filtre\n",
        "    for item in list(filters):\n",
        "      line = line.replace(item, '')\n",
        "\n",
        "    # passage du texte en minuscule\n",
        "    line = line.lower()\n",
        "\n",
        "    # suppression des mots contenant des chiffres\n",
        "    line = ' '.join([word for word in line.split(' ') if word.isalpha()])\n",
        "\n",
        "    new_lines.append(line)\n",
        "  return new_lines\n",
        "\n",
        "clean_small_train_texts = clean_textlines(small_train_texts)\n",
        "print(clean_small_train_texts)\n",
        "\n",
        "dict_words, index_words = get_words(clean_small_train_texts)\n",
        "print(dict_words)\n",
        "print(len(dict_words))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this was an absolutely terrible movie dont be lured in by christopher walken or michael ironside both are great actors but this must simply be their worst role in history even their great acting could not redeem this movies ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the columbian rebels were making their cases for revolutions maria conchita alonso appeared phony and her pseudolove affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actors like christopher walkens good name i could barely sit through it', 'i have been known to fall asleep during films but this is usually due to a combination of things including really tired being warm and comfortable on the sette and having just eaten a lot however on this occasion i fell asleep because the film was rubbish the plot development was constant constantly slow and boring things seemed to happen but with no explanation of what was causing them or why i admit i may have missed part of the film but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else i cant recommend this film at all', 'mann photographs the alberta rocky mountains in a superb fashion and jimmy stewart and walter brennan give enjoyable performances as they always seem to do br br but come on hollywood a mountie telling the people of dawson city yukon to elect themselves a marshal yes a marshal and to enforce the law themselves then gunfighters battling it out on the streets for control of the town br br nothing even remotely resembling that happened on the canadian side of the border during the klondike gold rush mr mann and company appear to have mistaken dawson city for deadwood the canadian north for the american wild westbr br canadian viewers be prepared for a reefer madness type of enjoyable howl with this ludicrous plot or to shake your head in disgust', 'this is the kind of film for a snowy sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big armchair and mellow for a couple of hours wonderful performances from cher and nicolas cage as always gently row the plot along there are no rapids to cross no dangerous waters just a warm and witty paddle through new york life at its best a family film in every sense and one that deserves the praise it received', 'as others have mentioned all the women that go nude in this film are mostly absolutely gorgeous the plot very ably shows the hypocrisy of the female libido when men are around they want to be pursued but when no men are around they become the pursuers of a year old boy and the boy becomes a man really fast we should all be so lucky at this age he then gets up the courage to pursue his true love']\n",
            "{'this': 12, 'was': 6, 'an': 2, 'absolutely': 2, 'terrible': 1, 'movie': 3, 'dont': 1, 'be': 5, 'lured': 1, 'in': 7, 'by': 1, 'christopher': 2, 'walken': 2, 'or': 3, 'michael': 1, 'ironside': 1, 'both': 1, 'are': 6, 'great': 2, 'actors': 2, 'but': 7, 'must': 1, 'simply': 1, 'their': 3, 'worst': 1, 'role': 1, 'history': 1, 'even': 2, 'acting': 1, 'could': 2, 'not': 1, 'redeem': 1, 'movies': 2, 'ridiculous': 1, 'storyline': 1, 'is': 3, 'early': 1, 'nineties': 1, 'us': 1, 'propaganda': 1, 'piece': 1, 'the': 29, 'most': 1, 'pathetic': 2, 'scenes': 1, 'were': 2, 'those': 1, 'when': 4, 'columbian': 1, 'rebels': 1, 'making': 1, 'cases': 1, 'for': 8, 'revolutions': 1, 'maria': 1, 'conchita': 1, 'alonso': 1, 'appeared': 1, 'phony': 1, 'and': 14, 'her': 1, 'pseudolove': 1, 'affair': 1, 'with': 4, 'nothing': 2, 'a': 16, 'emotional': 1, 'plug': 1, 'that': 5, 'devoid': 1, 'of': 15, 'any': 2, 'real': 2, 'meaning': 1, 'i': 8, 'am': 1, 'disappointed': 1, 'there': 2, 'like': 2, 'ruining': 1, 'walkens': 1, 'good': 1, 'name': 1, 'barely': 1, 'sit': 1, 'through': 2, 'it': 4, 'have': 4, 'been': 1, 'known': 1, 'to': 12, 'fall': 1, 'asleep': 2, 'during': 2, 'films': 1, 'usually': 1, 'due': 1, 'combination': 1, 'things': 2, 'including': 1, 'really': 2, 'tired': 1, 'being': 1, 'warm': 2, 'comfortable': 1, 'on': 5, 'sette': 1, 'having': 1, 'just': 3, 'eaten': 1, 'lot': 1, 'however': 1, 'occasion': 1, 'fell': 1, 'because': 1, 'film': 6, 'rubbish': 1, 'plot': 4, 'development': 1, 'constant': 1, 'constantly': 1, 'slow': 1, 'boring': 1, 'seemed': 2, 'happen': 2, 'no': 4, 'explanation': 1, 'what': 1, 'causing': 1, 'them': 1, 'why': 1, 'admit': 1, 'may': 1, 'missed': 1, 'part': 1, 'watched': 1, 'majority': 1, 'everything': 1, 'its': 3, 'own': 2, 'accord': 1, 'without': 1, 'concern': 1, 'anything': 1, 'else': 1, 'cant': 1, 'recommend': 1, 'at': 3, 'all': 3, 'mann': 2, 'photographs': 1, 'alberta': 1, 'rocky': 1, 'mountains': 1, 'superb': 1, 'fashion': 1, 'jimmy': 1, 'stewart': 1, 'walter': 1, 'brennan': 1, 'give': 1, 'enjoyable': 2, 'performances': 2, 'as': 4, 'they': 3, 'always': 2, 'seem': 1, 'do': 1, 'br': 5, 'come': 1, 'hollywood': 1, 'mountie': 1, 'telling': 1, 'people': 1, 'dawson': 2, 'city': 2, 'yukon': 1, 'elect': 1, 'themselves': 2, 'marshal': 2, 'yes': 1, 'enforce': 1, 'law': 1, 'then': 2, 'gunfighters': 1, 'battling': 1, 'out': 1, 'streets': 1, 'control': 1, 'town': 1, 'remotely': 1, 'resembling': 1, 'happened': 1, 'canadian': 3, 'side': 1, 'border': 1, 'klondike': 1, 'gold': 1, 'rush': 1, 'mr': 1, 'company': 1, 'appear': 1, 'mistaken': 1, 'deadwood': 1, 'north': 1, 'american': 1, 'wild': 1, 'westbr': 1, 'viewers': 1, 'prepared': 1, 'reefer': 1, 'madness': 1, 'type': 1, 'howl': 1, 'ludicrous': 1, 'shake': 1, 'your': 1, 'head': 1, 'disgust': 1, 'kind': 1, 'snowy': 1, 'sunday': 1, 'afternoon': 1, 'rest': 1, 'world': 1, 'can': 1, 'go': 2, 'ahead': 1, 'business': 1, 'you': 1, 'descend': 1, 'into': 1, 'big': 1, 'armchair': 1, 'mellow': 1, 'couple': 1, 'hours': 1, 'wonderful': 1, 'from': 1, 'cher': 1, 'nicolas': 1, 'cage': 1, 'gently': 1, 'row': 1, 'along': 1, 'rapids': 1, 'cross': 1, 'dangerous': 1, 'waters': 1, 'witty': 1, 'paddle': 1, 'new': 1, 'york': 1, 'life': 1, 'best': 1, 'family': 1, 'every': 1, 'sense': 1, 'one': 1, 'deserves': 1, 'praise': 1, 'received': 1, 'others': 1, 'mentioned': 1, 'women': 1, 'nude': 1, 'mostly': 1, 'gorgeous': 1, 'very': 1, 'ably': 1, 'shows': 1, 'hypocrisy': 1, 'female': 1, 'libido': 1, 'men': 2, 'around': 2, 'want': 1, 'pursued': 1, 'become': 1, 'pursuers': 1, 'year': 1, 'old': 1, 'boy': 2, 'becomes': 1, 'man': 1, 'fast': 1, 'we': 1, 'should': 1, 'so': 1, 'lucky': 1, 'age': 1, 'he': 1, 'gets': 1, 'up': 1, 'courage': 1, 'pursue': 1, 'his': 1, 'true': 1, 'love': 1}\n",
            "299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQrhD8l68zx1"
      },
      "source": [
        "# 3 Formatage des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fekv1vBz826-"
      },
      "source": [
        "## 3.1 Encodage vectoriel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2FpCTCDcq8U",
        "outputId": "86683fe1-93da-4a92-f52d-c0263e5712e0"
      },
      "source": [
        "def vector_count_representation(lines, id_words):\n",
        "\n",
        "  nb_words = len(id_words)\n",
        "  nb_lines = len(lines)\n",
        "\n",
        "  vrep_lines = np.zeros((nb_lines, nb_words))\n",
        "  for i,line in enumerate(lines):\n",
        "      words = line.split(' ')\n",
        "      for word in words:\n",
        "        vrep_lines[i,id_words[word]-1] += 1\n",
        "\n",
        "  return vrep_lines\n",
        "\n",
        "vector_rep_st = vector_count_representation(clean_small_train_texts, index_words)\n",
        "print(vector_rep_st.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 299)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lcNcz6k86oA"
      },
      "source": [
        "## 3.2 Encodage séquentiel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVh3qm3sVTtQ",
        "outputId": "d102f911-c17b-4eb9-fdfb-cec12fccd63d"
      },
      "source": [
        "def sequential_representation(lines, id_words):\n",
        "\n",
        "  vseq_lines = []\n",
        "  for i,line in enumerate(lines):\n",
        "    seq = [id_words[word] for word in line.split(' ')]\n",
        "    vseq_lines.append(seq)\n",
        "\n",
        "  return vseq_lines\n",
        "\n",
        "sequential_rep_st = sequential_representation(clean_small_train_texts, index_words)\n",
        "print(sequential_rep_st)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 1, 22, 23, 8, 24, 25, 26, 10, 27, 28, 24, 19, 29, 30, 31, 32, 1, 33, 34, 35, 1, 6, 36, 3, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 42, 49, 50, 46, 51, 24, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 13, 2, 65, 21, 66, 44, 67, 68, 10, 66, 6, 69, 2, 70, 71, 72, 73, 74, 75, 76, 77, 69, 78, 18, 33, 79, 1, 80, 20, 79, 12, 81, 82, 83, 75, 30, 84, 85, 86, 87], [75, 88, 89, 90, 91, 92, 93, 94, 95, 21, 1, 36, 96, 97, 91, 66, 98, 71, 99, 100, 101, 102, 103, 104, 60, 105, 106, 42, 107, 60, 108, 109, 110, 66, 111, 112, 106, 1, 113, 75, 114, 93, 115, 42, 116, 2, 117, 42, 118, 119, 2, 120, 121, 122, 60, 123, 99, 124, 91, 125, 21, 64, 126, 127, 71, 128, 2, 129, 130, 14, 131, 75, 132, 75, 133, 88, 134, 135, 71, 42, 116, 21, 75, 136, 42, 137, 71, 87, 60, 138, 109, 124, 91, 125, 71, 139, 140, 141, 142, 72, 73, 143, 53, 144, 145, 75, 146, 147, 1, 116, 148, 149], [150, 151, 42, 152, 153, 154, 10, 66, 155, 156, 60, 157, 158, 60, 159, 160, 161, 162, 163, 164, 165, 166, 167, 91, 168, 169, 169, 21, 170, 106, 171, 66, 172, 173, 42, 174, 71, 175, 176, 177, 91, 178, 179, 66, 180, 181, 66, 180, 60, 91, 182, 42, 183, 179, 184, 185, 186, 87, 187, 106, 42, 188, 53, 189, 71, 42, 190, 169, 169, 65, 28, 191, 192, 69, 193, 106, 42, 194, 195, 71, 42, 196, 94, 42, 197, 198, 199, 200, 150, 60, 201, 202, 91, 88, 203, 175, 176, 53, 204, 42, 194, 205, 53, 42, 206, 207, 208, 169, 194, 209, 8, 210, 53, 66, 211, 212, 213, 71, 162, 214, 64, 1, 215, 118, 14, 91, 216, 217, 218, 10, 219], [1, 36, 42, 220, 71, 116, 53, 66, 221, 222, 223, 48, 42, 224, 71, 42, 225, 226, 227, 228, 64, 139, 140, 229, 164, 230, 231, 232, 66, 233, 234, 60, 235, 53, 66, 236, 71, 237, 238, 163, 239, 240, 60, 241, 242, 164, 166, 243, 244, 42, 118, 245, 78, 18, 126, 246, 91, 247, 126, 248, 249, 109, 66, 104, 60, 250, 251, 86, 252, 253, 254, 148, 139, 255, 66, 256, 116, 10, 257, 258, 60, 259, 69, 260, 42, 261, 87, 262], [164, 263, 88, 264, 149, 42, 265, 69, 227, 266, 10, 1, 116, 18, 267, 4, 268, 42, 118, 269, 270, 271, 42, 272, 71, 42, 273, 274, 48, 275, 18, 276, 165, 277, 91, 8, 278, 21, 48, 126, 275, 18, 276, 165, 279, 42, 280, 71, 66, 281, 282, 283, 60, 42, 283, 284, 66, 285, 101, 286, 287, 288, 149, 8, 289, 290, 148, 1, 291, 292, 184, 293, 294, 42, 295, 91, 296, 297, 298, 299]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv9HrQfb8_aY"
      },
      "source": [
        "## 3.3 Base de validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ99HwranmXe"
      },
      "source": [
        "def extract_valid_data(x_train_data, y_train_data, valid_proportion=0.2):\n",
        "\n",
        "  split_point = int(len(x_train_data) * valid_proportion)\n",
        "  x_valid = x_train_data[:split_point]\n",
        "  x_train = x_train_data[split_point:]\n",
        "\n",
        "  y_valid = y_train_data[:split_point]\n",
        "  y_train = y_train_data[split_point:]\n",
        "\n",
        "  return x_train, y_train, x_valid, y_valid\n",
        "\n",
        "valid_proportion = .2\n",
        "x_train_small, y_train_small, x_valid, y_valid = extract_valid_data(vector_rep_st, small_train_labels, valid_proportion=valid_proportion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WNpAeMXXIJJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK7v9MUP9Cfq"
      },
      "source": [
        "## 3.4 Formatage des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiCkWX6btvKr"
      },
      "source": [
        "y_train_small = np.asarray(y_train_small).astype('float32')\n",
        "y_valid = np.asarray(y_valid).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPaGVggo9GFM"
      },
      "source": [
        "# 4 Réseau de neurones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opwGKKxu9bRw"
      },
      "source": [
        "\n",
        "4.1 Réseau feedforward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vvs-0igUp12O",
        "outputId": "2f08b2f5-b7d4-4315-fdb6-158a289141c6"
      },
      "source": [
        "def model_mlp(num_words):\n",
        "\n",
        "  input_layer = Input(shape=(num_words,))\n",
        "  dense1 = Dense(64, activation='relu')(input_layer)\n",
        "  dense2 = Dense(16, activation='relu')(dense1)\n",
        "  dense3 = Dense(1, activation='sigmoid')(dense2)\n",
        "  model = Model(input_layer, dense3)\n",
        "  model.compile(optimizer='sgd',\n",
        "              loss='binary_crossentropy', # si plus de deux classes: loss='categorical_crossentropy'\n",
        "              metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "my_model_mlp = model_mlp(len(index_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_12 (InputLayer)       [(None, 299)]             0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 64)                19200     \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 16)                1040      \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,257\n",
            "Trainable params: 20,257\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlR9hkwDtesN",
        "outputId": "bd2fc998-9509-44a0-bcf0-d336c190a1a1"
      },
      "source": [
        "history = my_model_mlp.fit(x=x_train_small, y=y_train_small,\n",
        "                    epochs=20, batch_size=4,\n",
        "                    validation_data=(x_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1/1 [==============================] - 1s 578ms/step - loss: 0.7614 - accuracy: 0.2500 - val_loss: 0.6372 - val_accuracy: 1.0000\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6782 - accuracy: 0.5000 - val_loss: 0.6298 - val_accuracy: 1.0000\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6129 - accuracy: 1.0000 - val_loss: 0.6227 - val_accuracy: 1.0000\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.5526 - accuracy: 1.0000 - val_loss: 0.6189 - val_accuracy: 1.0000\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.5031 - accuracy: 1.0000 - val_loss: 0.6177 - val_accuracy: 1.0000\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4626 - accuracy: 1.0000 - val_loss: 0.6226 - val_accuracy: 1.0000\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4295 - accuracy: 1.0000 - val_loss: 0.6256 - val_accuracy: 1.0000\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.4013 - accuracy: 1.0000 - val_loss: 0.6283 - val_accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.3757 - accuracy: 1.0000 - val_loss: 0.6301 - val_accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.3521 - accuracy: 1.0000 - val_loss: 0.6317 - val_accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.3298 - accuracy: 1.0000 - val_loss: 0.6343 - val_accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.3096 - accuracy: 1.0000 - val_loss: 0.6367 - val_accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.2911 - accuracy: 1.0000 - val_loss: 0.6388 - val_accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.2737 - accuracy: 1.0000 - val_loss: 0.6413 - val_accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.2578 - accuracy: 1.0000 - val_loss: 0.6438 - val_accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2436 - accuracy: 1.0000 - val_loss: 0.6459 - val_accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2305 - accuracy: 1.0000 - val_loss: 0.6489 - val_accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.2186 - accuracy: 1.0000 - val_loss: 0.6517 - val_accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2076 - accuracy: 1.0000 - val_loss: 0.6543 - val_accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1978 - accuracy: 1.0000 - val_loss: 0.6553 - val_accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4sDyomQ9YjA"
      },
      "source": [
        "## 4.2 Réseau récurrent simple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-rn4hQLlG1P",
        "outputId": "ef4863a0-b22b-4c17-81df-789ddaf8dac8"
      },
      "source": [
        "T_max = max([len(x) for x in sequential_rep_st])\n",
        "x_train_seq = pad_sequences(sequential_rep_st, maxlen=T_max, padding='post', truncating='post')\n",
        "print(T_max)\n",
        "\n",
        "x_train_small, y_train_small, x_valid, y_valid = extract_valid_data(x_train_seq, small_train_labels, valid_proportion=valid_proportion)\n",
        "y_train_small = np.asarray(y_train_small).astype('float32')\n",
        "y_valid = np.asarray(y_valid).astype('float32')\n",
        "print(x_train_small.shape)\n",
        "#x_train_small = np.reshape(x_train_small, (4, 131, 1))\n",
        "print(x_train_small.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "131\n",
            "(4, 131)\n",
            "(4, 131)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvXyhKBOkqUu",
        "outputId": "c4e4c09e-075a-48b7-dc71-630ac11c508d"
      },
      "source": [
        "def model_rnn(T_max):\n",
        "\n",
        "  input_layer = Input(shape=(T_max,1))\n",
        "  srnn1 = SimpleRNN(64, return_sequences=True)(input_layer)\n",
        "  srnn2 = SimpleRNN(32, return_sequences=True)(srnn1)\n",
        "  srnn3 = SimpleRNN(1, activation='sigmoid')(srnn2)\n",
        "  model = Model(input_layer, srnn3)\n",
        "  model.compile(optimizer='sgd',\n",
        "              loss='binary_crossentropy', # si plus de deux classes: loss='categorical_crossentropy'\n",
        "              metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "my_model_rnn = model_rnn(T_max)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_13 (InputLayer)       [(None, 131, 1)]          0         \n",
            "                                                                 \n",
            " simple_rnn_12 (SimpleRNN)   (None, 131, 64)           4224      \n",
            "                                                                 \n",
            " simple_rnn_13 (SimpleRNN)   (None, 131, 32)           3104      \n",
            "                                                                 \n",
            " simple_rnn_14 (SimpleRNN)   (None, 1)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,362\n",
            "Trainable params: 7,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McjEymicdoaS",
        "outputId": "ad73a06f-981e-4d94-e63d-36b61ceef3a0"
      },
      "source": [
        "history = my_model_rnn.fit(x=x_train_small, y=y_train_small,\n",
        "                    epochs=10, batch_size=4,\n",
        "                    validation_data=(x_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7367 - accuracy: 0.5000 - val_loss: 0.4725 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.5585 - accuracy: 1.0000 - val_loss: 0.4771 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.4311 - accuracy: 1.0000 - val_loss: 0.4898 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.3627 - accuracy: 1.0000 - val_loss: 0.4897 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3130 - accuracy: 1.0000 - val_loss: 0.4914 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.2775 - accuracy: 1.0000 - val_loss: 0.4845 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.2504 - accuracy: 1.0000 - val_loss: 0.4760 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.2283 - accuracy: 1.0000 - val_loss: 0.4658 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.2099 - accuracy: 1.0000 - val_loss: 0.4552 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.1943 - accuracy: 1.0000 - val_loss: 0.4449 - val_accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1JbESddEn1X"
      },
      "source": [
        "# 5 Application sur l’ensemble du jeu de données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12QkE1Pj33ut",
        "outputId": "21b802d2-fc2d-4706-85b0-72b979e86ca1"
      },
      "source": [
        "clean_train_texts = clean_textlines(train_texts)\n",
        "dict_words, index_words = get_words(clean_train_texts)\n",
        "print(len(dict_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "117382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl0P9d69ErgB"
      },
      "source": [
        "## 5.1 Réduction du vocabulaire"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlIem1Rt4tH6",
        "outputId": "3fd340e0-e824-422b-96a5-df2274d3b73b"
      },
      "source": [
        "def get_frequent_words(dict_words, n_freq_words, train_texts):\n",
        "  list_value_key = [(v, k) for k,v in dict_words.items()]\n",
        "  print(list_value_key[:10])\n",
        "  freq_words = sorted(list_value_key, reverse=True)[:n_freq_words]\n",
        "  print(freq_words[:10])\n",
        "  _, selected_words = zip(*freq_words)\n",
        "\n",
        "  reduced_text = []\n",
        "  for line in train_texts:\n",
        "    line = ' '.join([word if word in selected_words else 'unk' for word in line.split(' ')])\n",
        "    reduced_text.append(line)\n",
        "\n",
        "  return reduced_text\n",
        "\n",
        "reduced_train_texts = get_frequent_words(dict_words, 250, clean_train_texts)\n",
        "reduced_dict_words, reduced_index_words = get_words(reduced_train_texts)\n",
        "print(len(reduced_dict_words))\n",
        "for i in range(5):\n",
        "  print(reduced_train_texts[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(75189, 'this'), (48007, 'was'), (21486, 'an'), (1481, 'absolutely'), (1585, 'terrible'), (41803, 'movie'), (8471, 'dont'), (26630, 'be'), (28, 'lured'), (93024, 'in')]\n",
            "[(334678, 'the'), (162210, 'and'), (161936, 'a'), (145323, 'of'), (135041, 'to'), (106854, 'is'), (93024, 'in'), (77084, 'it'), (75717, 'i'), (75189, 'this')]\n",
            "251\n",
            "this was an unk unk movie dont be unk in by unk unk or unk unk both are great actors but this must unk be their worst role in unk even their great acting could not unk this movies unk unk this movie is an unk unk us unk unk the most unk scenes were those when the unk unk were making their unk for unk unk unk unk unk unk and her unk unk with unk was nothing but a unk unk unk in a movie that was unk of any real unk i am unk that there are movies like this unk actors like unk unk good unk i could unk unk through it\n",
            "i have been unk to unk unk unk films but this is unk unk to a unk of things unk really unk being unk and unk on the unk and unk just unk a lot however on this unk i unk unk because the film was unk the plot unk was unk unk unk and unk things unk to unk but with no unk of what was unk them or why i unk i may have unk part of the film but i unk the unk of it and unk just unk to unk of its own unk without any real unk for anything unk i cant unk this film at all\n",
            "unk unk the unk unk unk in a unk unk and unk unk and unk unk give unk unk as they always unk to do br br but come on unk a unk unk the people of unk unk unk to unk unk a unk unk a unk and to unk the unk unk then unk unk it out on the unk for unk of the unk br br nothing even unk unk that unk on the unk unk of the unk unk the unk unk unk unk unk and unk unk to have unk unk unk for unk the unk unk for the unk unk unk br unk unk be unk for a unk unk unk of unk unk with this unk plot or to unk your unk in unk\n",
            "this is the kind of film for a unk unk unk when the unk of the world can go unk with its own unk as you unk into a big unk and unk for a unk of unk unk unk from unk and unk unk as always unk unk the plot unk there are no unk to unk no unk unk just a unk and unk unk through new unk life at its best a family film in every unk and one that unk the unk it unk\n",
            "as unk have unk all the unk that go unk in this film are unk unk unk the plot very unk unk the unk of the unk unk when unk are around they want to be unk but when no unk are around they unk the unk of a unk old unk and the unk unk a man really unk we should all be so unk at this unk he then gets up the unk to unk his unk love\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec8SM0D4-e8v",
        "outputId": "10f7a735-f5ed-4d56-bbc1-5673bf608a5a"
      },
      "source": [
        "# MLP sur des représentations vectorielles\n",
        "vect_rep = vector_count_representation(reduced_train_texts, reduced_index_words)\n",
        "x_train, y_train, x_valid, y_valid = extract_valid_data(vect_rep, train_labels, valid_proportion=valid_proportion)\n",
        "y_train = np.asarray(y_train).astype('float32')\n",
        "y_valid = np.asarray(y_valid).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')\n",
        "print(x_train.shape)\n",
        "my_model_mlp = model_mlp(len(reduced_index_words))\n",
        "history_mlp = my_model_mlp.fit(x=x_train, y=y_train,\n",
        "                    epochs=10, batch_size=32,\n",
        "                    validation_data=(x_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(20000, 251)\n",
            "Model: \"model_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_14 (InputLayer)       [(None, 251)]             0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 64)                16128     \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 16)                1040      \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17,185\n",
            "Trainable params: 17,185\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 2s 2ms/step - loss: 0.7066 - accuracy: 0.5365 - val_loss: 0.6724 - val_accuracy: 0.6052\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.6757 - accuracy: 0.5969 - val_loss: 0.6635 - val_accuracy: 0.6126\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.6618 - accuracy: 0.6167 - val_loss: 0.6463 - val_accuracy: 0.6350\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.6495 - accuracy: 0.6396 - val_loss: 0.6825 - val_accuracy: 0.5946\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.6356 - accuracy: 0.6576 - val_loss: 0.6091 - val_accuracy: 0.6810\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.6289 - accuracy: 0.6691 - val_loss: 0.6291 - val_accuracy: 0.6528\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.6205 - accuracy: 0.6805 - val_loss: 0.5840 - val_accuracy: 0.7050\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.6155 - accuracy: 0.6827 - val_loss: 0.5841 - val_accuracy: 0.7084\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.6079 - accuracy: 0.6897 - val_loss: 0.5968 - val_accuracy: 0.6674\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.5892 - accuracy: 0.7034 - val_loss: 0.5672 - val_accuracy: 0.7008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VathuD7V-1Qa",
        "outputId": "cd35a8f6-b5d3-4357-b7ee-f7001d803f34"
      },
      "source": [
        "# RNN sur des représentations séquentielles\n",
        "seq_rep = sequential_representation(reduced_train_texts, reduced_index_words)\n",
        "T_max = min(max([len(x) for x in seq_rep]), 80)\n",
        "print(T_max)\n",
        "x_train, y_train, x_valid, y_valid = extract_valid_data(seq_rep, train_labels, valid_proportion=valid_proportion)\n",
        "x_train = pad_sequences(x_train, maxlen=T_max, padding='post', truncating='post')\n",
        "x_valid = pad_sequences(x_valid, maxlen=T_max, padding='post', truncating='post')\n",
        "y_train = np.asarray(y_train).astype('float32')\n",
        "y_valid = np.asarray(y_valid).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_valid.shape)\n",
        "my_model_rnn = model_rnn(T_max)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "80\n",
            "(20000, 80)\n",
            "(5000, 80)\n",
            "Model: \"model_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_15 (InputLayer)       [(None, 80, 1)]           0         \n",
            "                                                                 \n",
            " simple_rnn_15 (SimpleRNN)   (None, 80, 64)            4224      \n",
            "                                                                 \n",
            " simple_rnn_16 (SimpleRNN)   (None, 80, 32)            3104      \n",
            "                                                                 \n",
            " simple_rnn_17 (SimpleRNN)   (None, 1)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,362\n",
            "Trainable params: 7,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy1kEY-GcshB",
        "outputId": "c7e728b1-624e-4829-8763-511007d3e649"
      },
      "source": [
        "history_rnn = my_model_rnn.fit(x=x_train, y=y_train,\n",
        "                    epochs=10, batch_size=32,\n",
        "                    validation_data=(x_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 31s 47ms/step - loss: 0.6963 - accuracy: 0.5110 - val_loss: 0.6918 - val_accuracy: 0.5160\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.6930 - accuracy: 0.5130 - val_loss: 0.6933 - val_accuracy: 0.5132\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 0.6925 - accuracy: 0.5174 - val_loss: 0.6935 - val_accuracy: 0.5052\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 0.6919 - accuracy: 0.5196 - val_loss: 0.6925 - val_accuracy: 0.5244\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 27s 43ms/step - loss: 0.6918 - accuracy: 0.5158 - val_loss: 0.6938 - val_accuracy: 0.5128\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 0.6916 - accuracy: 0.5203 - val_loss: 0.6936 - val_accuracy: 0.5162\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 0.6915 - accuracy: 0.5199 - val_loss: 0.6939 - val_accuracy: 0.5054\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 0.6910 - accuracy: 0.5278 - val_loss: 0.6929 - val_accuracy: 0.5190\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 0.6911 - accuracy: 0.5222 - val_loss: 0.6938 - val_accuracy: 0.5200\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 0.6907 - accuracy: 0.5253 - val_loss: 0.6952 - val_accuracy: 0.5136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSE9EfI1ExEc"
      },
      "source": [
        "## 5.2 Application"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9u4LoLxIK4z",
        "outputId": "0954b3fd-15ff-4e70-812d-06f8bb34b908"
      },
      "source": [
        "def model_gru(T_max):\n",
        "\n",
        "  input_layer = Input(shape=(T_max,1))\n",
        "  srnn1 = GRU(64, return_sequences=True)(input_layer)\n",
        "  srnn2 = GRU(32, return_sequences=True)(srnn1)\n",
        "  srnn3 = GRU(1, activation='sigmoid')(srnn2)\n",
        "  model = Model(input_layer, srnn3)\n",
        "  model.compile(optimizer='sgd',\n",
        "              loss='binary_crossentropy', # si plus de deux classes: loss='categorical_crossentropy'\n",
        "              metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "my_model_gru = model_gru(T_max)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_16 (InputLayer)       [(None, 80, 1)]           0         \n",
            "                                                                 \n",
            " gru_5 (GRU)                 (None, 80, 64)            12864     \n",
            "                                                                 \n",
            " gru_6 (GRU)                 (None, 80, 32)            9408      \n",
            "                                                                 \n",
            " gru_7 (GRU)                 (None, 1)                 105       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,377\n",
            "Trainable params: 22,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSrQEZr2Ij4a",
        "outputId": "c071fe69-a702-4563-ce62-cf2fff1c3e36"
      },
      "source": [
        "history_gru = my_model_gru.fit(x=x_train, y=y_train,\n",
        "                    epochs=10, batch_size=32,\n",
        "                    validation_data=(x_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 50s 74ms/step - loss: 0.6941 - accuracy: 0.5100 - val_loss: 0.6919 - val_accuracy: 0.5204\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 46s 73ms/step - loss: 0.6937 - accuracy: 0.5084 - val_loss: 0.6919 - val_accuracy: 0.5112\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 46s 73ms/step - loss: 0.6932 - accuracy: 0.5121 - val_loss: 0.6921 - val_accuracy: 0.5096\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 45s 73ms/step - loss: 0.6935 - accuracy: 0.5074 - val_loss: 0.6916 - val_accuracy: 0.5162\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 45s 73ms/step - loss: 0.6927 - accuracy: 0.5145 - val_loss: 0.6914 - val_accuracy: 0.5200\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 45s 73ms/step - loss: 0.6929 - accuracy: 0.5179 - val_loss: 0.6960 - val_accuracy: 0.5066\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 46s 73ms/step - loss: 0.6931 - accuracy: 0.5100 - val_loss: 0.6923 - val_accuracy: 0.5112\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 45s 73ms/step - loss: 0.6927 - accuracy: 0.5138 - val_loss: 0.6916 - val_accuracy: 0.5142\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 45s 73ms/step - loss: 0.6926 - accuracy: 0.5128 - val_loss: 0.6916 - val_accuracy: 0.5186\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 45s 73ms/step - loss: 0.6923 - accuracy: 0.5157 - val_loss: 0.6914 - val_accuracy: 0.5212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTWwmJ7WuhUb",
        "outputId": "544c7049-2c6e-4636-d1b5-6017935b5f02"
      },
      "source": [
        "def model_gru_better(T_max, n_words, emb_dim):\n",
        "\n",
        "  input_layer = Input(shape=(T_max,))\n",
        "  mask_layer = Masking(mask_value=0.0)(input_layer)\n",
        "  embedding_layer = Embedding(input_dim=n_words,     # taille du vocabulaire\n",
        "                              output_dim=emb_dim,    # taille du vecteur de mots\n",
        "                              input_length=T_max)(mask_layer) # taille d'un texte)(input_layer)\n",
        "  srnn1 = GRU(32, return_sequences=True)(embedding_layer)\n",
        "  srnn2 = GRU(16, return_sequences=True)(srnn1)\n",
        "  flatten_layer = Flatten()(srnn2)\n",
        "  dense1 = Dense(32, activation='relu')(flatten_layer)\n",
        "  dense2 = Dense(1, activation='sigmoid')(dense1)\n",
        "  #srnn3 = GRU(1, activation='sigmoid')(srnn2)\n",
        "  model = Model(input_layer, dense2)\n",
        "  model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy', # si plus de deux classes: loss='categorical_crossentropy'\n",
        "              metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "my_model_gru_better = model_gru_better(T_max, len(dict_words), 128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_17 (InputLayer)       [(None, 80)]              0         \n",
            "                                                                 \n",
            " masking_3 (Masking)         (None, 80)                0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 80, 128)           15024896  \n",
            "                                                                 \n",
            " gru_8 (GRU)                 (None, 80, 32)            15552     \n",
            "                                                                 \n",
            " gru_9 (GRU)                 (None, 80, 16)            2400      \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 1280)              0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 32)                40992     \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,083,873\n",
            "Trainable params: 15,083,873\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fHPbLB6vQLR",
        "outputId": "6c4dff0e-df26-4c75-d462-b16863f9a25e"
      },
      "source": [
        "history_gru_better = my_model_gru_better.fit(x=x_train, y=y_train,\n",
        "                    epochs=10, batch_size=32,\n",
        "                    validation_data=(x_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 69s 105ms/step - loss: 0.6037 - accuracy: 0.6568 - val_loss: 0.5570 - val_accuracy: 0.7040\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 65s 104ms/step - loss: 0.5556 - accuracy: 0.7078 - val_loss: 0.5496 - val_accuracy: 0.7156\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 65s 104ms/step - loss: 0.5356 - accuracy: 0.7211 - val_loss: 0.5575 - val_accuracy: 0.7150\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 65s 104ms/step - loss: 0.5141 - accuracy: 0.7391 - val_loss: 0.5700 - val_accuracy: 0.7180\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 65s 104ms/step - loss: 0.4915 - accuracy: 0.7553 - val_loss: 0.5787 - val_accuracy: 0.6980\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 65s 104ms/step - loss: 0.4655 - accuracy: 0.7714 - val_loss: 0.5984 - val_accuracy: 0.6942\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 65s 104ms/step - loss: 0.4321 - accuracy: 0.7946 - val_loss: 0.6269 - val_accuracy: 0.6934\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 65s 104ms/step - loss: 0.3987 - accuracy: 0.8147 - val_loss: 0.6561 - val_accuracy: 0.6804\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 65s 104ms/step - loss: 0.3601 - accuracy: 0.8396 - val_loss: 0.7193 - val_accuracy: 0.6752\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 65s 104ms/step - loss: 0.3177 - accuracy: 0.8609 - val_loss: 0.8038 - val_accuracy: 0.6780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4m4Diyg3o-E",
        "outputId": "f9ad8510-8e27-4c89-d915-8cad92783c07"
      },
      "source": [
        "clean_test_texts = clean_textlines(test_texts)\n",
        "reduced_test_texts = get_frequent_words(dict_words, 250, clean_test_texts)\n",
        "test_vect_rep = vector_count_representation(reduced_test_texts, reduced_index_words)\n",
        "test_seq_rep = sequential_representation(reduced_test_texts, reduced_index_words)\n",
        "test_seq_rep = pad_sequences(test_seq_rep, maxlen=T_max, padding='post', truncating='post')\n",
        "print(\"MLP\")\n",
        "print(my_model_mlp.evaluate(test_vect_rep, y_test))\n",
        "print(\"RNN vanilla\")\n",
        "print(my_model_rnn.evaluate(test_seq_rep, y_test))\n",
        "print(\"GRU\")\n",
        "print(my_model_gru.evaluate(test_seq_rep, y_test))\n",
        "print(\"GRU OPTIM\")\n",
        "print(my_model_gru_better.evaluate(test_seq_rep, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(75189, 'this'), (48007, 'was'), (21486, 'an'), (1481, 'absolutely'), (1585, 'terrible'), (41803, 'movie'), (8471, 'dont'), (26630, 'be'), (28, 'lured'), (93024, 'in')]\n",
            "[(334678, 'the'), (162210, 'and'), (161936, 'a'), (145323, 'of'), (135041, 'to'), (106854, 'is'), (93024, 'in'), (77084, 'it'), (75717, 'i'), (75189, 'this')]\n",
            "MLP\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5699 - accuracy: 0.7027\n",
            "[0.569901704788208, 0.7027199864387512]\n",
            "RNN vanilla\n",
            "782/782 [==============================] - 11s 14ms/step - loss: 0.6960 - accuracy: 0.5066\n",
            "[0.695966899394989, 0.5066400170326233]\n",
            "GRU\n",
            "782/782 [==============================] - 17s 22ms/step - loss: 0.6923 - accuracy: 0.5143\n",
            "[0.6922540068626404, 0.5143200159072876]\n",
            "GRU OPTIM\n",
            "101/782 [==>...........................] - ETA: 10s - loss: 0.8066 - accuracy: 0.6782"
          ]
        }
      ]
    }
  ]
}